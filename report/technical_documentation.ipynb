{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10240345,"sourceType":"datasetVersion","datasetId":6326521}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Inspired from \n# https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training a Temporal Convolutional Network on ISRUC-Sleep Dataset\n\nIn this tutorial, we will guide you through the process of training a Temporal Convolutional Network (TCN) for time series classification using the **ISRUC-Sleep** dataset. The steps outlined here focus on preparing the dataset, building the model, and training and evaluating the network.\n\n### Prerequisites\nBefore starting, ensure that you have the following:\n- A P100 GPU (or any other GPU) for model training.\n- Python 3.x installed along with PyTorch, torchvision, and pandas libraries.\n\n### Overview\nWe will go through the following steps:\n1. **Load and Normalize the ISRUC Processed Dataset**: Use PyTorch utilities to handle data loading and preprocessing.\n2. **Define the Temporal Convolutional Neural Network (TCN)**: Create a custom neural network for time series classification.\n3. **Define the Loss Function**: Set up a loss function appropriate for classification.\n4. **Train the Network**: Train the TCN on the training dataset.\n5. **Evaluate the Network**: Test the trained model on the test dataset.\n\n---\n\n## 1. Loading and Normalizing the ISRUC Processed Dataset\n\n### Dataset Overview\n\nThe **ISRUC-Sleep** dataset consists of time series data that has been preprocessed and converted into `.csv` files, which are ideal for time series classification tasks. You can find the dataset on the following platforms:\n\n- [Original ISRUC-Sleep Dataset](https://sleeptight.isr.uc.pt/)\n- [Kaggle Datacard](https://www.kaggle.com/datasets/rishitjakharia/isruc-sg1)\n- [Processed Dataset on Kaggle](https://www.kaggle.com/datasets/rishitjakharia/isruc-processed)\n\nFor this tutorial, we will use the **processed version** of the dataset.\n\n![Polysomnography for Apnea](https://www.cancertherapyadvisor.com/wp-content/uploads/sites/12/2019/01/ch2606.fig4_.png)\n\n### Loading Data with Pandas\n\nFirst, load the processed dataset into a Pandas DataFrame. Assume the data is stored in CSV format.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load training and testing datasets\ntrain_data = pd.read_csv('/kaggle/input/isruc-processed/dataset/Events/oa/S1_p100_1_Stagen1_Event4_Session1.csv')\n\n# Display basic information about the dataset\nprint(train_data.head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Converting Data to PyTorch Tensors\n\nTo feed the data into the neural network, we need to convert the Pandas DataFrames into PyTorch tensors. We'll use `torch.tensor()` for this conversion.","metadata":{}},{"cell_type":"code","source":"import torch\n\n# Convert the training and testing data to PyTorch tensors\ntrain_tensor = torch.tensor(train_data.values, dtype=torch.float32)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Normalizing the Data\n\nFor neural networks to perform optimally, data normalization is essential. We'll use the mean and standard deviation of the training data to normalize both the training and testing datasets.","metadata":{}},{"cell_type":"code","source":"# Normalize the data by subtracting the mean and dividing by the standard deviation\nmean = train_tensor.mean()\nstd = train_tensor.std()\n\ntrain_tensor = (train_tensor - mean) / std","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Creating a Custom Dataset Class\n\nWe will create a custom PyTorch `Dataset` class that loads the data in batches. A Simplified version of the Dataset class implemented in the `.ipynb` file is shown below.","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\n\nclass EventClassificationDataset(Dataset):\n    def __init__(self, root_dir, transform=None, label_mapping=None, included_classes=None):\n        \"\"\"\n        Args:\n            root_dir (str): Root directory containing all folders and signal files.\n            transform (callable, optional): Optional transform to apply to the signals.\n            included_classes (list, optional): List of classes to include. If None, include all classes.\n        \"\"\"\n        self.root_dir = root_dir\n        self.transform = transform\n        self.included_classes = included_classes\n        self.label_mapping = label_mapping\n        self.data_info = self._prepare_file_list()\n\n    def _prepare_file_list(self):\n        \"\"\"Scan the dataset directory and prepare a list of file paths and labels.\"\"\"\n        data_info = []\n\n        event_path = os.path.join(self.root_dir, 'Events')\n        if os.path.isdir(event_path):\n            for label_folder in os.listdir(event_path):\n                if self.included_classes and label_folder not in self.included_classes:\n                    continue\n\n                label_path = os.path.join(event_path, label_folder)\n                if os.path.isdir(label_path):\n                    for file_name in os.listdir(label_path):\n                        # Full file path\n                        file_path = os.path.join(label_path, file_name)\n                        data_info.append((file_path, label_folder))\n\n        # Process Non Event files\n        non_event_path = os.path.join(self.root_dir, 'Non_Events')\n        if os.path.isdir(non_event_path):\n            for subfolder_name in os.listdir(non_event_path):\n                subfolder_path = os.path.join(non_event_path, subfolder_name)\n                if os.path.isdir(subfolder_path):\n                    for file_name in os.listdir(subfolder_path):\n                        # Full file path\n                        file_path = os.path.join(subfolder_path, file_name)\n\n                        # Label for non-events is \"no_event\"\n                        if not self.included_classes or 'no_event' in self.included_classes:\n                            data_info.append((file_path, 'no_event'))\n\n        return data_info\n\n    def __len__(self):\n        return len(self.data_info)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        # Get file path and label\n        file_path, label = self.data_info[idx]\n\n        # Convert label to one-hot encoding\n        unique_labels = self.included_classes if self.included_classes else ['ar', 'awake', 'ca', 'ch', 'l on', 'l out', \n                                                                             'lm', 'mchg', 'mh', 'oa', 'oh', 'plm', 'rem', \n                                                                             'no_event']\n        label_to_one_hot = {label: [1 if i == idx else 0 for i in range(len(unique_labels))] \n                            for idx, label in enumerate(unique_labels)}\n        label = label_to_one_hot[label]\n\n        possible_columns = [\n            ['X6', 'X7', 'X8', 'SaO2'],\n            ['X6', 'X7', 'X8', 'SpO2'],\n            ['29', '30', '31', 'SaO2'],\n            ['29', '30', '31', 'SpO2']\n        ]\n        \n        signal = pd.read_csv(file_path)\n    \n        for columns in possible_columns:\n            if all(col in signal.columns for col in columns):\n                signal = signal[columns]\n                break\n        \n        signal = torch.tensor(signal.values, dtype=torch.float32).T\n        if self.label_mapping:\n            label = self.label_mapping[np.argmax(label)]\n        label = torch.tensor(label, dtype=torch.long)\n        \n        if self.transform:\n            for transform in self.transform:\n                signal = transform(signal)\n\n        return signal.T, label","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The label mapping defined as done below, maps the 6 classes of types of apnea and hypopnea events into two classes namely `A/H Event` or `Normal Breathing`.\n\nFurthermore, the index of the classes is the same as specified in `included_classes`. Hence, if we wished to convert the 6 class output to 3 classes namely `Apnea`, `Hypopnea` and `Normal Breathing` we would use a code similar to the one showed below in the markdown.\n\n```python\nlabel_mapping = [[1,0,0], [1,0,0], \n                 [0,1,0], [0,1,0], [0,1,0],\n                 [0,0,1]\n                ]\n```","metadata":{}},{"cell_type":"code","source":"root_dir = '/kaggle/input/isruc-processed/dataset'\nincluded_classes = ['ca', 'oa', 'oh', 'mh', 'ch', 'no_event']\n\nlabel_mapping = [[1,0], [1,0], \n                 [1,0], [1,0], [1,0],\n                 [0,1]\n                ]\n\nevent_classification_dataset = EventClassificationDataset(root_dir=root_dir, label_mapping=label_mapping, included_classes=included_classes)\nevent_classification_loader = DataLoader(event_classification_dataset, batch_size=64, shuffle=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Creating a Train Test Split\n\nSince we are dealing with Medical Data, we must ensure that there is no data leakage between the train and test subsets of the dataset to achieve this we use the patient information stored in the name of the dataset. \n> The structure is specified in the discription of the dataset.","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nfrom sklearn.model_selection import train_test_split\n\npatient_session_groups = defaultdict(list)\n\nfor idx, (_, label) in enumerate(event_classification_dataset.data_info):\n    filename = event_classification_dataset.data_info[idx][0]\n    filename = filename.split(\"/\")[-1]\n    patient_session_key = \"_\".join(filename.split(\"_\")[:2])  # e.g., \"S1_p_1\"\n    patient_session_groups[patient_session_key].append(idx)\n\ngroup_keys = list(patient_session_groups.keys())\ntrain_group_keys, test_group_keys = train_test_split(\n    group_keys, test_size=0.2, random_state=14\n)\n\n# Get train and test indices from groups\ntrain_idx = [idx for key in train_group_keys for idx in patient_session_groups[key]]\ntest_idx = [idx for key in test_group_keys for idx in patient_session_groups[key]]\n\n# Create train and test subsets\ntrain_subset = torch.utils.data.Subset(event_classification_dataset, train_idx)\ntest_subset = torch.utils.data.Subset(event_classification_dataset, test_idx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 64\ntrain_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n\nprint(f\"Number of training samples: {len(train_subset)}\")\nprint(f\"Number of testing samples: {len(test_subset)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 2. Defining the Temporal Convolutional Network (TCN)\n\nThe Temporal Convolutional Network (TCN) is an effective architecture for time-series data, using dilated convolutions to model long-range dependencies. \n\nBelow is a simplified version of the complete Temporal Conv Net used for the classification.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass TCNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, dilation_rate):\n        super(TCNBlock, self).__init__()\n\n        self.conv1 = nn.Conv1d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            padding='same',\n            dilation=dilation_rate\n        )\n        self.batch_norm = nn.BatchNorm1d(out_channels)\n\n        # Residual connection\n        if in_channels != out_channels:\n            self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n        else:\n            self.shortcut = None\n\n    def forward(self, x):\n        out = F.relu(self.conv1(x))\n        out = self.batch_norm(out)\n\n        shortcut = self.shortcut(x) if self.shortcut is not None else x\n        return F.relu(out + shortcut)\n\nclass TCNModel(nn.Module):\n    def __init__(self, n_length, n_features, n_outputs):\n        super(TCNModel, self).__init__()\n\n        self.tcn_blocks = nn.Sequential(\n            TCNBlock(n_features, 32, kernel_size=3, dilation_rate=1),\n            TCNBlock(32, 64, kernel_size=3, dilation_rate=2)\n        )\n\n        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n        self.fc1 = nn.Linear(64, 32)\n        self.fc2 = nn.Linear(32, n_outputs)\n\n        self.dropout = nn.Dropout(0.3)\n\n    def forward(self, x):\n        x = x.permute(0, 2, 1)  # Convert to (batch_size, n_features, n_length)\n        x = self.tcn_blocks(x)\n\n        x = self.global_avg_pool(x).squeeze(-1)  # Global average pooling\n\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n\n        return F.softmax(x, dim=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 3. Defining the Loss Function\n\nFor this demonstration, we will use **cross-entropy loss**, which is common for multi-class classification problems. However, we have used custom loss function in the final code.","metadata":{}},{"cell_type":"code","source":"# Define the loss function (Cross Entropy for classification)\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 4. Training the Network\n\nWe'll now proceed to train the network. During training, we will use an optimizer (e.g., Adam) to update the model parameters.","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\n\n# Initialize the model, loss function, and optimizer\nn_timesteps = 30 * 12.5 # Seconds Per Epoch * Sampling Frequency\nn_features = 4 # No. of features used\nn_outputs = 2\n\n# Create the model\nmodel = TCNModel(n_length=n_timesteps, n_features=n_features, n_outputs=n_outputs)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training loop\nnum_epochs = 1\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_data, batch_labels in train_loader:\n        batch_labels = batch_labels.float()  # Ensure labels are in the correct dtype\n        \n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(batch_data)\n        loss = criterion(outputs, batch_labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## 5. Evaluating the Network\n\nAfter training, it's crucial to evaluate the model on unseen data. We will check the model's performance on the test set.","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test set\nmodel.eval()  # Set model to evaluation mode\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for batch_data, batch_labels in test_loader:\n        batch_labels = batch_labels.float()\n\n        # Forward pass\n        outputs = model(batch_data)\n        _, predicted = torch.max(outputs, 1)\n        \n        # Calculate accuracy\n        total += batch_labels.size(0)\n        correct += (predicted == batch_labels).sum().item()\n\naccuracy = 100 * correct / total\nprint(f\"Test Accuracy: {accuracy:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n## Conclusion\n\nIn this tutorial, we demonstrated how to:\n- Load and preprocess the ISRUC-Sleep dataset.\n- Define a Temporal Convolutional Network (TCN) for time series classification.\n- Train the network and evaluate its performance.\n\nBy following these steps, you can use PyTorch to build and train time-series classifiers on similar datasets.","metadata":{}}]}